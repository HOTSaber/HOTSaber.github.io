# pyTorch环境安装及配置；

1、为什么下载anaconda：里面包含了很多包，集成了大量科学分析的库的软件
2、显卡的作用：提升模型的运算速度，同时使得模型的大小可以更大
3、如何管理pytorch的版本：同时保存多种pytorch，如何更新卸载版本，创建虚拟环境
如何进入特定房间，即环境
如何查看pytorch里的包
4、如何查看GPU的型号：可以不用管
如何再cmd查看显卡情况，nvidia-smi
驱动版本过低怎么办：在nvidia驱动程序更新
5、如何判断pytorch安装成功，以及cuda是否可用

pycharm和jupyter的使用
6、pycharm如何创建一个新的conda环境的工程文件
7、python如何进行单行代码运行，类似matlab的操作，便于入门和test的处理
选择console

8、jupeter的特点，可用单步运行，容易找小错误，可以交互，以单元格的方式逐个执行代码
Jupyter Notebook的代码补全和错误检查功能相对较弱
pycharm的特点，可用集成大规模代码的运行，但是pycharm也可以单步运行，比如一个class很长的那种

9、如何pytorch环境安装jupeter，使得jupeter可以使用pytorch

# package结构

1、dir函数和help函数的使用



pycharm和jupyter的使用区别：jupyter一般是小的项目
1、运行过程的区别
2、体验的区别
3、各自的优缺点

PyTorch的数据加载及实战：
1、实现数据集的打包（往往对一系列文件的数据导入打包）
2、从哪里导入dataset（torch.utils.data）
3、用jupeter使用help更清晰

4、dataset是干嘛的
5、dataload是干嘛的
相对位置读取的时候./和../的区别
断点的意义
shulffe的意义，默认值是什么
droplast的意义，默认值是什么


4、用self的意义，相当于全局变量，可以给类中所有函数使用
5、os的作用意义，读入文件地址

# TensorBoard监视器的使用

1、TensorBoard监视器主要对标量和图片等进行查看，这里主要不要画图的时候同名，之前画的图不会删除
writer写入新事件的时候，原来的事件会保留
2、监视器一方面可以看到每次打包的数据集的内容
3、监视器可以查看模型的训练过程，实时的可视化
尤其是做图像处理的内容，可以可视化每次数据增强的结果，帮助判断优化空间



Transforms的使用:数据增强
conpose的含义



# nn.Module的使用

1.神经网络必须继承于他
2.卷积的操作是怎么实现的，卷积核的步长是什么意思，空洞卷积是什么意思
3.reshape是什么意思 修改形状，使数据符合模型的输入

4.可以通过监视器（Tensorboard）看到模型的结构，主要用于下载下来的模型熟悉其结构

`writer.add_graph()`

5.反向传播的时候为什么梯度要设置为0：
梯度设置为0是为了准备接收新的梯度值，因为在反向传播过程中，梯度是逐层累加的，如果不将梯度置为0，则上一次的梯度会继续累积到当前的梯度中，导致梯度计算出现错误。

6.哪些数据需要转移到GPU运算：
loss，model，输入

7.训练过程中可以利用监视器查看训练进程

`writer.add_scalar()`

看后面：现有网络的读取使用（例如RNN、VGG）、修改、添加、删除网络模型，并在监视器查看修改情况

模型中debug的使用，可以查看各种变量在过程中的变化，以及各种属性



虚拟环境配置 安装不同的库 显卡是加速器，多个GPU，可以并行运算

查看环境：conda env list

jupyter pycharm 控制台 也可以查看各种变量

jupyter优点：可以逐行输出结果，适合复杂模型调试。可以单步运行，适合找错。可以把每次的输出结果保留。

控制台：逐步运行

pycharm：运行全部

os库用于读取修改文件路径

对于图片这种需要单独存取的，用dataset读取

#self

self，在别的方法中使用定义的方法 ,实例化，所有类中的方法都可以直接调用

# nn.Module 

所有神经网络需要继承这个父类，这个父类为所有神经网络提供了基本的骨架（定义 方法）

# tensorboard

 运行监视器 可视化输出每次运行结果 显示划分的测试集和训练集 与原始模型进行对比，显示修改模型后的效果 **可以帮助画出模型架构图**

torchvision torch库中自带的数据集 `from torchvision.dataset import`+数据库

transforms 转换 数据增强 

![image-20230627200829584](C:\Users\翁鹏通\AppData\Roaming\Typora\typora-user-images\image-20230627200829584.png)

totensor 便于在后续神经网络计算框架中使用

数据增强，

由于python版本不同，有些包没了，去找到这个包放到对应目录下

# debug使用

断点：经过运行的行会显示变量的属性

​			debug运行到这行

![image-20230704200222983](第六次会议/image-20230704200222983.png)



###预训练模型

`model = torchvision.models.shufflenet_v2_x1_0(pretrained=True)`

`in_features = model.fc.in_features`

`model.fc = nn.Linear(infeature,2)`

![image-20230704200255973](第六次会议/image-20230704200255973.png)

#监视器

（画出神经网络的结构图）`writer.add_graph(model, dummy_input)`

# 线性层、嵌入层、激活层、dropout层

##激活函数

调用方式在：`self.relu = nn.ReLU()`

## 线性层

`self.fc = nn.Linear(int, int)`

## dropout层

`self.drop = nn.Dropout()`使一部分参数不进行梯度运算

## 嵌入层

输入必须使整数，且非负数字

作用：增加模型特征

`self.embedding = nn.Embedding(10,3)`，10表示有对应表示的数字个数，3表示用三维向量表示一个数值

问题：10表示什么？ 0~9都有对应的表示 相当于生成一个字典，0~9分别对应10个不同的三维向量，例如输入（1，2，3）转换为字典中对应的（三维向量，三维向量，三维向量）

self.embedding（input）样本的维度不能超过这个10【问题】

## Sequential(层，层，层)

按顺序进入各个层，前一个层的输出进入下一个层

# 模型



![image-20230704202936317](第六次会议/image-20230704202936317.png)

![image-20230704203230937](第六次会议/image-20230704203230937.png)

问题：为什么要梯度清零

![image-20230704203532203](第六次会议/image-20230704203532203.png)

梯度裁剪

定义的模型、损失函数、输入签要 .to(device)

# 哪些内容需要用GPU

## module

## 损失函数

##输入module的地方

# 模型的导入导出

##调度器

控制学习率

##![image-20230704204611716](第六次会议/image-20230704204611716.png)

##保存模型的权重参数![image-20230704204810090](第六次会议/image-20230704204810090.png)

## 调用模型（本地模型）

![image-20230704205017383](第六次会议/image-20230704205017383.png)

与预训练模型调用不同

# 模型训练步骤

##  1.模型与初始化

（1）模型（model）实例化

`from torchvision.models import vgg16, VGG16_Weights`

`model = vgg16(weights=VGG16_Weights.DEFAULT)`

（2）优化器（optim）初始化

`optimizer = optim.Adam(model.parameters(), lr=1e-3)`

（3）损失函数（loss function）初始化

`loss_f = nn.CrossEntropyLoss().to(device)`

## 2.数据准备

（1）训练、测试数据准备、打乱

```
dataset = torchvision.datasets.CIFAR10("../data", train=False, transform=torchvision.transforms.ToTensor(),
                                       download=True)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
```

## 3.训练过程（model.train）

（1）model_grad_zero:梯度清空

```
optimizer.zero_grad()
```

（2）训练数据输入得到train_output

```
for data in train_dataloader:
    imgs, targets = data
    # imgs = imgs.to(device)
    # targets =targets.to(device)
    outputs = model(imgs)
```

（3）计算损失loss

```
loss = loss_f(outputs, targets)
```

（4）反向计算梯度backforward

```
loss.backward()
```

（5）优化器更新参数optim.step

```
optimizer.step()
```

（6）**迭代**

## 4.测试过程(model.eval)

（1）去除梯度更新 with no_grad

```
with torch.no_grad():
```

（2）测试数据输入得到test_output

```
for data in test_dataloader:
    imgs, targets = data
    # imgs = imgs.to(device)
    # targets = targets.to(device)
    outputs = model(imgs)
```

（3）计算损失loss

```
loss = loss_f(outputs, targets)
```

（4）指标验证
自由定义acurracy函数如何设计。