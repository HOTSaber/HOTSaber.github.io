# 联邦学习

####*Communication-Efficient Federated Deep Learning with Asynchronous Model Update and Temporally Weighted Aggregation*（2019）

具有异步模型更新和时间加权聚合的高效通信联合深度学习

问题：联邦学习中的一个挑战是减少客户端-服务器通信，因为终端设备的通信带宽通常非常有限。

内容：本文通过**在客户端提出同步学习策略和在服务器上对本地模型进行时间加权聚合**，提出了一种增强的联邦学习技术。**在异步学习策略中，深度神经网络的不同层被分为浅层和深层，深层的参数更新频率低于浅层。**此外，在服务器上引入了时间加权聚合策略，以利用先前训练的本地模型，从而提高中心模型的准确性和收敛性。

结果：所提出的异步联邦深度学习在通信成本和模型准确性方面都优于基线算法。



####*Resource-Efficient Federated Learning with Hierarchical Aggregation in Edge Computing（2021）*

基于分层聚合的联邦学习的边缘计算 

问题：现有的 FL 机制可能会导致训练时间过长并消耗大量的通信资源。

内容：通过k-means聚类，将边缘节点划分为 K 个簇。一个簇中的边缘节点将它们的本地更新通过同步方法转发到簇头进行聚合，称为簇聚合，而所有簇头执行异步方法进行全局聚合。这种处理过程称为分层聚合。我们的分析表明收敛界限取决于集群的数量和训练时期。我们正式定义了**具有层次聚合 (RFL-HA) 问题的资源高效联邦学习**。我们提出了一种有效的算法来确定具有资源约束的最佳集群结构（即 K 的最佳值）并将其扩展以处理动态网络条件。

结果：从我们对不同模型和数据集的研究中获得的大量模拟结果表明，与众所周知的 FL 机制相比，所提出的算法可以将完成时间减少 34.8%70%，将通信资源减少 33.8%-56.5%，同时达到相似的精度.



####*Toward Resource-Efficient Federated Learning in Mobile Edge Computing（2021）*

在移动边缘计算中实现资源高效的联邦学习

问题：联邦学习在移动边缘计算中的瓶颈是移动客户端在计算、带宽、能量和数据方面的资源密集

内容：本文研究联邦学习中最先进的资源优化方法。联邦学习的资源高效技术大致分为两类：黑盒和白盒方法。黑盒分为训练技巧、客户端选择、数据补偿和分层聚合。白盒分为模型压缩、知识蒸馏、特征融合和异步更新。**提出了一种基于模块的联邦学习的神经结构感知资源管理方法**，其中移动客户端根据其本地资源的状态分配给全局模型的不同子网。

结果：实验证明了我们的方法在弹性和高效资源利用方面的优越性。



####*MHAT: An efficient model-heterogenous aggregation training scheme for federated learning（2021）*

MHAT：一种高效的联邦学习模型异构聚合训练方案

问题：目前用于联邦学习的服务器聚合算法仅关注模型参数，导致通信成本高且收敛速度低。最重要的是，他们无法处理不同客户端拥有具有不同网络架构的不同本地模型的场景。

内容：我们关注应该聚合什么以及如何提高收敛效率。我们提出了 MHAT，**一种新的模型异构聚合训练联邦学习方案**，它利用知识蒸馏（KD）技术提取所有客户端异构模型的更新信息，并在服务器上训练辅助模型以实现信息聚合. MHAT 使客户不必固定在统一的模型架构上，并显着减少所需的计算资源，同时保持可接受的模型收敛精度。



####**Efficient Privacy-Preserving Federated Learning With Unreliable Users*（2022）

不可靠用户的高效隐私保护联邦学习

 问题：存在不可靠用户的负面影响，难以保证目标模型使用高质量数据进行更新。

内容：提出了一种具有不可靠用户的有效隐私保护 FL (EPPFL) 方案。**通过迭代执行我们的“排除无关组件”和“加权聚合”**，FL 模型可以快速收敛，同时占用有限的通信和计算开销。这样一来，不仅**可以优化模型精度，还可以提高训练效率**。同时，我们建立了一个**基于门限Paillier密码系统的安全框架**，可以在训练过程中严格**保护所有与用户相关的隐私信息**。



####**Improving Federated Learning With Quality-Aware User Incentive and Auto-Weighted Model Aggregation*（2022）

通过质量感知用户激励和自动加权模型聚合改进联邦学习

问题：由于各种因素（例如，训练数据大小、错误标记的数据样本、倾斜的数据分布），计算节点的模型更新质量可能会发生巨大变化，而包容性地聚合低质量模型更新可能会降低全局模型质量。

内容：我们提出了一个具有质量意识的联邦学习FAIR，它集成了三个主要组件：1）学习质量估计：我们采用**模型聚合权重**（在第三个组件中学习）**以隐私保护的方式反向量化节点的个体学习质量**，并利用历史学习记录来推断下一轮的学习质量； 2) 质量感知激励机制：在招聘预算内，我们**模拟了一个逆向拍卖（买方市场，采购者提供采购信息，卖方进行竞争）问题来激励高质量、低成本的计算节点参与**，该方法被证明是真实的、个体理性的和计算高效的； 3）自动加权模型聚合：基于梯度下降法，我们设计了一种自动加权模型聚合算法，自动学习最优聚合权重，进一步提升全局模型质量。



####*LightFed: An Efficient and Secure Federated Edge Learning System on Model Splitting*（2022）

LightFed：一种高效安全的模型分割联邦边缘学习系统

问题：人工智能能、物联网以及联合边缘学习的集成正在开发一种很有前途的计算框架。然而，**由于模型庞大，传输链路不可靠，在通信效率和数据安全方面仍存在未解决的问题**。

内容：本文提出了一种新的联邦边缘学习系统，称为 LightFed，其中**边缘节点仅上传重要的部分本地模型，并成功实现轻量级通信和模型聚合**。提出了一种新的模型聚合方法，包含新的模型拆分和拼接（MSS）和选择性参数传输（SPT）方案。通过检测局部参数的更新梯度，过滤重要参数，实现局部模型的选择性旋转传输和高效聚合。其次，提出了训练填充模型（TFM）来推断边缘节点的总数据分布，并训练填充模型以在不侵犯个人用户数据隐私的情况下减轻不平衡的训练数据。此外，还提出了一种基于区块链的混淆传输机制，以抵御来自外部对手的攻击并保护模型信息。



####*A two-phase half-async method for heterogeneity-aware federated learning*(2022)

一种用于异质性感知联邦学习的两阶段半异步方法

问题：在极端的统计和环境异质性下，现有方法仍然远非高效和稳定。

内容：我们提出了FedHA(联邦异质性感知)，**一种新颖的半异步算法，结合了异步和同步方法的优点**。**它通过估计收集的局部模型的优化方向的一致性将训练分为两个阶段**。它在这些阶段应用不同的策略来促进快速稳定的训练，即模型选择、自适应局部时期和异质性加权聚合。



####*High-efficient hierarchical federated learning on non-IID data with progressive collaboration*（2022）

具有渐进协作的非独立同分布数据的高效分层联邦学习

问题：为了解决联邦学习中非独立同分布数据和通信瓶颈问题，大多数分层联邦学习（HFL）算法都假设客户端可以分配给任何边缘设备，但这种假设不切实际。

内容：在本文中，我们提出了一种名为 FedPEC 的高效 HFL 算法，它引入了**渐进式边缘协作**，而不是不切实际的客户端分配。 FedPEC根据我们证明的收敛上界**估计初始协作者数量**，然后**在后续轮次中根据每个阶段的特点不断调整估计的协作者数量**。在估计的协作者数量的指导下，可以根据**自适应相似性阈值**为每个边缘设备分配适当的协作者集。



####*FedSup: A communication-efficient federated learning fatigue driving behaviors supervision approach*（2023）

FedSup：一种高效沟通的联邦学习疲劳驾驶行为监督方法

问题：车辆之间的数据共享可用于优化疲劳检测模型并确保驾驶安全。数据隐私问题阻碍了共享过程，此外，由于通信和计算资源的限制，很难在车辆上进行训练和数据传输。

内容：我们提出了 FedSup，这是一种用于疲劳驾驶行为监督的高效通信联邦学习方法。受边缘智能中资源分配机制的启发，FedSup 通过定制的**客户端-边缘-云架构动态优化共享模型**，并通过贝叶斯卷积神经网络 (BCNN) 数据选择策略减少通信开销**。**为了提高共享模型的优化效率，我们进一步提出了一种**异步参数聚合算法**来自动调整每个边缘模型参数的混合权重。



####*Communication-efficient semi-synchronous hierarchical federated learning with balanced training in heterogeneous IoT*（2023）

异构物联网中具有平衡训练的通信高效半同步分层联邦学习

问题：物联网环境在硬件、网络技术、网络条件和电源方面的固有异构性，对原始 FL 提出了与模型收敛速度、模型精度、平衡训练和通信成本相关的挑战。

内容：我们提出了一种**基于半同步通信的分层 FL (HFL) 框架**，以解决智能物联网边缘环境中**原生 FL 的异构性问题**。特别是，所提出的方法以分层方式运行本地模型的聚合过程以生成全局模型，并使用**半同步通信**来降低通信成本。此外，**引入加权聚合方法来克服原始 FL 在异构环境中训练不平衡的问题。**



####基于边缘的联邦学习模型清洗和设备聚类方法（2021）

本地模型可能是与全局模型收敛方向相反的恶意模型。本文研究了一种基于边缘的模型清洗和设备聚类方法,以减少本地更新总数。**通过计算本地更新参数和全局模型参数在多维上的余弦相似度来判断本地更新是否是必要的**。终端设备根据其所在的网络位置**聚类**,并通过移动边缘节点以簇的形式与云端通信,从而避免与服务器高并发访问相关的延迟



#### *边缘场景下动态权重的联邦学习优化方法（2022）

问题：设备异构、数据异质及通信方面的挑战，如模型偏移、收敛效果差、部分设备计算结果丢失等问题。

内容：**提出动态权重的联邦学习优化算法(FedDw)**。该算法**关注设备的服务质量**，减少训练速度不一致导致部分设备参与带来的异构性影响，并**根据服务质量确定在最终模型聚合时的占比**，从而确保聚合的结果在复杂的真实情况下更具有鲁棒性。



####移动边缘网络中联邦学习效率优化综述（2021）

从边缘协调与模型压缩的角度讨论分析了通信优化方案；从 设 备 选 择 、资 源 协 调 、聚 合 控 制 与 数 据 优 化 ４ 个方面讨论分析了训练优化方案



####面向数据异构的联邦学习性能优化研究（2023）（黄）

数据异构会导致本地模型与全局最优模型收敛方向不一致，影响全局模型性能。本文研究了一种**基于局部模型偏移的性能优化方案**，本地训练过程中结合所有客户端模型关键参数， 提高全局聚合模型可信度。具体来说，**计算待训练模型与其他客户端模型参数差值，然后乘以其他客户端梯度，将结果作为正则项加入本地损失函数**，从而抑制局部模型偏移。



#### 联邦学习隐私模型发布综述（2022）

本文针对联邦学习本地模型和聚合模型发布过程中可能出现的各种隐私威胁和敌手模型进行了简要介绍，并且对相关的防御技术和研究成果进行系统性综述．



#### 基于同态加密的高效安全联邦学习聚合框架（2023）

问题：在联邦学习模型训练的过程中采用加密方式带来计算和通信开销会影响训练效率

内容：采用 Top-K 梯度选择方法**对模型梯度进行筛选**，减少了需要上传的梯度数量，提出**适合多边缘节点的候选量化协议和安全候选索引合并算法**，进一步降低通信开销、加速同态加密计算。其次，由于神经网络每层模型参数具有高斯分布的特性，**对选择的模型梯度进行裁剪量化**，并采用梯度无符号量化协议以加速同态加密计算。



####基于联邦学习的入侵检测机制研究(2022)

文章首先介绍了联邦学习及入侵检测模型的构成及特点，提出了**基于联邦学习的入侵检测机制**，并深入分析了该检测机制在检测准确率及效率上有效提升的可行性。



####基于联邦学习的本地化差分隐私机制研究(2022)

问题：通过分析联邦学习模型训练后的参数仍然可能泄漏用户的隐私信息。目前运用本地化差分隐私保护模型参数的方法皆难以在较小的隐私预算和用户数量下缩小模型测试精度差。

内容：该文提出**正负分段的差分隐私扰动机制**，在聚合前对本地模型参数进行扰动。



####基于本地化差分隐私的联邦学习方法研究（2022）

问题：联邦学习训练过程中存在的推理攻击问题

内容：首先，**设计一种本地化差分隐私机制**，作用在联邦学习参数的传递过程中，保证联邦模型训练过程免受推理攻击的影响。其次，提出并设计一种**适用于联邦学习的性能损失约束机制**，通过优化损失函数的约束范围来降低本地化差分隐私联邦模型的性能损失。



***

第二次

####非独立同分布数据下的自正则化联邦学习优化方法 （2023）

问题：联邦学习中不同 客户端数据的非独立同分布问题，目前提出的一些解决方案没有利用好本地模型和全局模型的隐含关系，无法简单而高效地解决问题

内容：提出新的联邦学习优化算法——联邦自正则(FedSR)和动态联邦自正则(Dyn-FedSR)。FedSR在每轮训练中加入自正则化惩罚项动态修改本地损失函数，由此来构建本地模型与全局模型的关系，使得本地模型靠近全局模型，缓解 Non-IID 数据带来的客户端偏移问题。Dyn-FedSR 则在 FedSR 基础上通过计算本地模型 和全局模型的相似度动态确定正则项系数。



####基于相似度聚类的可信联邦安全聚合算法（2023）

问题：联邦学习模型训练中传递的参数或者梯度仍有可能泄露参与方的隐私数据，而恶意参与方的存在则会严重影响聚合过程和模型质量。

内容：本文提出一种基于相似度聚类的可信联邦安全聚合方法(FSA-SC)。基于客户端训练数据集规模及其与服务器间的通信距离综合评估选出拟参与模型聚合的候选客户端；然后根据候选客户端间的相似度，利用聚类将候选客户端划分为良性客户端和异常客户端；最后，对异常客户端类中的成员利用类内广播和二次协商进行参数替换和记录，检测识别恶意客户端。

 

####基于选择性通信策略的高效联邦学习研究（2023）

问题：高通信开销问题阻碍着联邦学习的进一步发展

内容：本文提出了基于选择性通信策略的高效联邦学习算法。该算法基于联邦学习的网络结构特点，采 取选择性通信策略，在客户端通过最大均值差异衡量本地模型与全局模型的相关性以过滤相关性较低的本 地模型，并在服务器端依据相关性对本地模型进行加权聚合。



####联邦学习安全防御与隐私保护技术研究（2022）

问题：由于联邦学习的固有缺陷以及存储和通信的安全问题，其在实际应用场景中仍面临多 种安全与隐私威胁。

内容：首先阐述了 ＦＬ 面临的安全攻击和隐私攻击；然后针对这两类典型攻击分别总结了最新的 安全防御机制和隐私保护手段，包括投毒攻击防御、后门攻击防御、搭便车攻击防御、女巫攻击防御以及基于安 全计算与差分隐私的防御手段。



#### 联邦学习模型安全与隐私研究进展*

内容：介绍联邦学习的背景知识, 明确其定义和工作流程, 并分析存在的脆弱点. 其次, 分 别对联邦学习存在的安全威胁和隐私风险进行系统分析和对比, 并归纳总结现有的防护手段



#### 联邦学习中的隐私保护技术∗

问题：然而联邦学习引入了大 量参数交换过程, 不仅和集中式训练一样受到模型使用者的威胁, 还可能受到来自不可信的参与设备的攻击, 因 此亟需更强的隐私手段保护各方持有的数据.

内容：分析并展望了联邦学习中的隐私保护技术的研究进展和趋势. 简要介绍联邦学习的架构和类型, 分析联邦学习过程中面临的隐私风险, 总结重建、推断两种攻击策略, 然后依据联邦学习中的隐私保护机制归纳隐私保护技术, 并深入调研应用上述技术的隐私保护算法, 从中心、本地、中心与本地结合这 3 个层面总结现有的保护策略.



#### 面向用户需求挖掘的去中心化异步联邦 LDA 算法

问题：新颁布的隐私保护法规对用户需求挖掘算法提出了要求。

内容：本文提出了面向用户需求挖掘的去中心化异步联邦隐狄利克雷分布算法 DAFedLDA。基于对等分布式 LDA，进一步提出了基于多 链的权限控制机制 MCACS 以及基于随机丢弃的数据贡献质量监控机制 RDDMS。



#### 融合边缘智能计算和联邦学习的隐私保护方案

问题：联邦学习会暴露边缘智能终端的训练集信息

内容：本文提出了一种**融合边缘智能计算和联邦学习的隐私保护方案(PPCEF）**，提出了一个基于共享秘密和权重掩码的轻量级隐私保护协议，可以保护梯度隐私和抵抗设备掉线和设备间的共谋攻击。此外，还设计了一种基于数字签名和哈希函数的算法，不仅可以实现消息的完整性和一致性，还能抵抗重放攻击。



#### 一种基于背景优化的高效联邦学习方案

问题：由于非独立同分布数据的存在以及数据量不平衡、数据类型不平衡等问题，客户端在利用本地数据进行 训练时不可避免地存在精确度缺失、训练效率低下等问题。

内容：提出了一种基于背景优化的高效联邦学习方案，用于提高终端设备中本地模型的精确度，在不同的环境中根据精确度的差异性来选择第一设备和第二设备，将第一设备模型和全局模型的 不 相 关 性 （下文统称为差异值）作为标准差异值；而第二设备是否上传本地模型则由第二设备和第一设备之间的差异值决定。

不足：在未来的研究中，我们将考虑联邦学习环境中的更多可 变因素，如设备的对比的随机性、设备的不稳定性、可靠性等



#### 一种异步联邦学习聚合更新算法

问题：当用户本地训练速度相差较大时，采用指数滑动平均法作为参数聚合更新方法无法消除由此造成的聚合参 数偏差，从而显著影响模型整体训练效率．

内容：本文提出了一种**基于权重摘要和更新版本感知的异步联邦学习聚合更新方法**，通过**合理控制不同训练速度用户提交的参数在聚合参数中所占比例**，以及主动更新落后用户使用的聚合参数，从而有效**解决本地训练速度差异对聚合参数造成的负面影响**。



####A Learning-based Incentive Mechanism for Federated Learning（2020）

一种基于学习的联邦学习激励机制

问题：联邦学习大多数现有工作都集中在设计具有经过验证的收敛时间的学习算法，未能探索激励机制，它不能直接应用于联邦学习。

内容：研究了联邦学习的激励机制，以激励边缘节点贡献模型训练。具体来说，设计了一种基于深度强化学习（DRL）的激励机制来确定参数服务器的最优定价策略和边缘节点的最优训练策略。



####Adaptive Federated Learning with Gradient Compression in Uplink NOMA（2020）

上行链路 NOMA 中具有梯度压缩的自适应联邦学习

问题：FL中的模型更新会遇到性能瓶颈，尤其在分布式网络中训练深度学习模型时。

内容：研究了通过无线链路连接到参数服务器 (PS) 的移动边缘设备的 FL 更新性能。考虑到无线衰落信道的频谱限制，我们进一步利用非正交多址接入 (NOMA) 以及自适应梯度量化和稀疏化来促进高效的上行链路 FL 更新。

保持精度前提下，可以显着降低 FL 聚合延迟。

 

#### Federated Learning with Differential Privacy: Algorithms and Performance Analysis（2020）

具有差分隐私的联邦学习：算法和性能分析

问题：联邦学习中信息仍可能通过客户端上传的参数泄漏。

内容：提出了一种基于差分隐私 (DP) 概念的新框架，其中在聚合之前向客户端的参数添加人工噪声。并且提出了三个差分隐私保护的理论界限：1）收敛性能和隐私保护级别之间存在折衷，即更好的收敛性能导致更低的保护级别； 2）给定固定的隐私保护级别，增加参与FL的整体客户端数量N可以提高收敛性能； 3) 就给定保护级别的收敛性能而言，存在最佳聚合次数（通信轮数）。提出了一种 K-client 随机调度策略，其中 K (1 ≤ K < N ) 个客户端从 N 个整体客户端中随机选择参与每个聚合。我们还为这种情况下的损失函数制定了相应的收敛界限



####Incentive Mechanism for Reliable Federated Learning: A Joint Optimization Approach to Combining Reputation and Contract Theory(2019)

可靠联邦学习的激励机制：一种结合声誉和契约理论的联合优化方法

问题：参与培训的激励机制和用于可靠联邦学习的工作人员（即移动设备）选择方案等挑战尚未得到探索。

内容：我们首先引入声誉作为衡量移动设备可靠性和可信度的指标。然后，我们通过使用多权重主观逻辑模型为可靠的联邦学习设计基于信誉的工人选择方案。我们还利用区块链以去中心化的方式为具有不可否认性和防篡改属性的工人实现安全的声誉管理。此外，我们提出了一种将声誉与契约理论相结合的有效激励机制，以激励具有高质量数据的高声誉移动设备参与模型学习。数值结果清楚地表明，就显着提高学习准确性而言，所提出的方案对于可靠的联邦学习是有效的。



#### Privacy-Preserving Collaborative Deep Learning with Unreliable Participants（2019）

具有不可靠参与者的隐私保护协作深度学习

内容：提出了一个实用的隐私保护协作深度学习系统，在不直接进行数据共享和中央数据存储的前提下，允许用户使用所有参与者的数据进行集体深度学习模型的训练。所有参与者用自己的数据训练本地的模型，只共享模型参数，并采用差分隐私的方式来进一步避免共享模型参数带来的隐私泄漏问题。



#### Privacy-Preserving Federated Deep Learning with Irregular Users（2020）

问题：在联邦训练过程中，许多不规则用户共享的数据可能会损害训练的准确性

内容：提出了 PPFDL，这是一种**具有不规则用户的隐私保护联合深度学习框架**，来减少不规则用户对训练准确性的负面影响。采用乱码电路和加法同态密码系统来确保所有与用户相关的信息的机密性。PPFDL对在整个实施过程中退出的用户也具有鲁棒性，训练过程中有用户离线，剩下的在线用户仍然可以完成训练任务。



####Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data（2020）

问题：联邦学习形式的隐私保护协作学习会产生训练期间大量的通信开销。现有的有关分布式训练提出的几种压缩信息量的方法在联邦学习的环境中作用有限，因为它们要么只压缩从客户端到服务器的上游通信（不压缩下游通信），要么只在理想化条件下表现良好。

内容：提出了稀疏三元压缩（STC），这是一种新的压缩框架，专门设计用于满足联邦学习环境的要求。STC 通过一种新机制扩展了现有的 top-k 梯度稀疏化压缩技术，以实现下游压缩以及权重更新的三元化和最佳 Golomb 编码。



####Communication-efficient hierarchical federated learning for IoT heterogeneous systems with imbalanced data(2022)

具有不平衡数据的物联网异构系统的高效通信分层联邦学习

问题：由于隐私问题和严重的通信瓶颈，将更新的 FL 模型发送到中央服务器可能变得不切实际。

内容：本文研究了分层 FL 在物联网 (IoT) 异构系统中的潜力。特别是，我们为物联网异构系统的分层 FL 架构提出了用户分配和资源分配的优化解决方案。



####Convergence of Edge Computing and Deep Learning: A Comprehensive Survey（2020）

边缘计算与深度学习的融合：综合调查

内容：针对互惠智能和智能边缘，本文介绍和讨论：1）两者的应用场景； 2) 实用的实现方法和使能技术，即定制化边缘计算框架中的深度学习训练和指示； 3）更普适和细粒度智能的挑战和未来趋势。



#### Non-IID data and Continual Learning processes in Federated Learning: A long road ahead（2022）

联邦学习中的非独立同分布数据和持续学习过程：前路漫漫

问题：联邦学习这种分布式方法很容易受到统计数据异质性的影响，无论是在不同实体之间还是随着时间的推移，这可能会导致缺乏收敛性。

内容：我们对统计数据异质性进行了正式分类，并回顾了能够应对它的最卓越的学习联邦学习策略。在本文中，我们提出了许多可以轻松适应联邦学习设置以提高其性能的方法。持续学习策略值得特别关注，因为它们能够处理习惯性的数据异质性。除了从理论上讨论数据异质性的负面影响外，我们还使用不同类型的非 IID 数据对其进行了检验并展示了一些实证结果。

