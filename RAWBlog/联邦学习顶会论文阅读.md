###Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering（ICML,2022）

通过保持全局一致的聚类实现无监督的联邦学习

问题：联邦学习用于无监督学习需要一些特性：1.对于统计 or 系统异质性要具有鲁棒性 2.参与者数量要有可扩展性 3.考虑通信效率

内容：我们提出了Orchestra，一种新型的无监督的联合学习技术，**利用联合的层次结构来协调分布式聚类任务，并强制将客户的数据全局一致地划分为可区分的聚类**。



### ProgFed: Effective, Communication, and Computation Efficient Federated Learning by Progressive Training（ICML，2022）

通过渐进式训练实现有效通信、计算的联邦学习

问题：边缘设备的训练受网络宽带的约束

内容：我们提出了ProgFed，这是第一个用于高效和有效的联合学习的渐进训练框架。它本质上减少了计算和双向通信成本，同时保持了最终模型的强大性能。



### Heterogeneity for the Win: One-Shot Federated Clustering（ICML，2021）

利用异质性取胜：一次性联邦聚类

内容：对于具有异质性的系统，有特别的优势，强调一次性聚类的实用性



### *FEDSPEED: LARGER LOCAL INTERVAL, LESS COMMUNICATION ROUND, AND HIGHER GENERALIZATION ACCURACY（ICLR，2023

FEDSPEED：更大的局部间隔，更少的通信轮次，更高的泛化精度

问题：联邦学习的性能收到本地不一致的最优引入的非消失偏差以及本地过拟合的客户端的影响。

内容：在本文中，我们提出了一种新颖而实用的方法--FedSpeed，以缓解这些问题所带来的负面影响。具体来说，FedSpeed**将近似修正项应用于当前的局部更新**，以有效地减少由近似项引入的偏差，这是保持强局部一致性的一个必要的正则器。此外，FedSpeed将虚无缥缈的随机梯度与邻域的额外梯度上升步骤计算的扰动合并，从而缓解了局部过拟合的问题。



### EPISODE: EPISODIC GRADIENT CLIPPING WITH PERIODIC RESAMPLED CORRECTIONS FOR FEDERATED LEARNING WITH HETEROGENEOUS DATA（ICLR,2023)

EPISODE：针对异质数据联邦学习的周期性重采样校正的情节**梯度裁剪**

问题：异质数据联邦学习，目前还不清楚如何在具有异质数据和有限通信回合的一般联邦学习（FL）环境中设计可证明有效的梯度剪裁算法。

内容：在本文中，我们设计了EPISODE，这是第一个在非凸和宽松的平滑度环境下解决异质数据的FL问题的算法。该算法的关键成分是两种新技术，即偶发梯度剪裁和定期重采样修正。在每一轮开始时，EPISODE对每个客户的随机梯度进行重采样，并获得全局平均梯度，用于(1)确定是否对整个回合应用梯度剪裁，(2)为每个客户构建局部梯度修正。



### DEPTHFL: DEPTHWISE FEDERATED LEARNING FOR HETEROGENEOUS CLIENTS（ICLR，2023）

DEPTHFL：异构客户端的深度联邦学习

问题：本地客户端的资源是异构的，对于资源受限的客户端使用宽度缩放技术，在聚合通道时会遇到通道参数不匹配的问题，导致精度低于简单地将资源受限的客户端排除在培训之外。

内容：本文提出了一种**基于深度缩放**的新方法 DepthFL 来解决这个问题。 **DepthFL 通过修剪全局模型的最深层来定义不同深度的局部模型，并根据客户端的资源将它们分配给客户端。**由于许多客户没有足够的资源来训练深度局部模型，这会使深层在数据不足的情况下进行部分训练，这与完全训练的浅层不同。 **DepthFL通过局部模型中不同深度的分类器之间的知识相互自蒸馏来缓解这个问题**。



### Communication-Efficient Adaptive Federated Learning（ICLR，2023）

有效沟通的自适应联邦学习

问题：由于重复的服务器-客户端同步导致的大量通信开销以及基于 SGD 的模型更新缺乏适应性

内容：在本文中，我们提出了一种具有理论收敛性保证的新型通信高效自适应联邦学习方法 (FedCAMS)。我们表明，在非凸随机优化设置中，我们提出的 FedCAMS 实现了与其非压缩对应物相同的收敛速度 O( 1 √T Km )。